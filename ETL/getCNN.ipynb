{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime as d\n",
    "\n",
    "import lxml\n",
    "import requests as r\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get links of all categories\n",
    "def getCatLinks():\n",
    "    main_url = 'http://edition.cnn.com/'\n",
    "    res = r.get(main_url)\n",
    "    res.encoding='unicode'\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    tags = soup.select('.m-footer__bucket_group .m-footer__link')\n",
    "    \n",
    "    catList = []\n",
    "    for tag in tags:\n",
    "        if (tag['href'].split('.')[0] == '//money'):\n",
    "            catList.append('http:%s'%(tag['href']))\n",
    "        elif (tag['href'].split(':')[0] != 'http'):\n",
    "            catList.append(main_url + '%s'%(tag['href'].lstrip('/')))\n",
    "    return catList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[DEBUG]\n",
    "for link in getCatLinks():\n",
    "    res = r.get(link)\n",
    "    if res.status_code == 200:\n",
    "        print('[ok!]' + link)\n",
    "    else:\n",
    "        print('[%s Fail!]'%(res.status_code) + link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get all links of articles in each category\n",
    "def getArticlesList(cat_url):\n",
    "    \n",
    "    articlesList = []\n",
    "\n",
    "    res = r.get(cat_url)\n",
    "    res.encoding='unicode'\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    titles = soup.select('.cd__headline a')\n",
    "    \n",
    "    for article in titles:\n",
    "        art_info = {}\n",
    "        art_info['title'] = article.text \n",
    "        \n",
    "        if (article['href'].lstrip('/').split(':')[0] == 'https'):\n",
    "            art_info['url'] = article['href'].lstrip('/')\n",
    "        elif (article['href'].lstrip('/').split(':')[0] == 'http'):\n",
    "            art_info['url'] = article['href'].lstrip('/')\n",
    "        else:\n",
    "            art_info['url'] = 'http://edition.cnn.com/' + article['href'].lstrip('/')\n",
    "        \n",
    "        articlesList.append(art_info)\n",
    "        \n",
    "    return articlesList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[DEBUG]\n",
    "for catLink in getCatLinks()[0:1]:\n",
    "    print(len(getArticlesList(catLink)))\n",
    "    \n",
    "    for art in getArticlesList(catLink):\n",
    "        res = r.get(art['url'])\n",
    "        if res.status_code == 200:\n",
    "            print('[ok!]' + art['title'])\n",
    "        else:\n",
    "            print('[%s Fail!]'%(res.status_code) + art['url'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getArticlesList(getCatLinks()[0])[10]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a signle article content\n",
    "def getArticle(art_url):\n",
    "    \n",
    "    art_dict = {}\n",
    "\n",
    "    res = r.get(art_url)\n",
    "    res.encoding='unicode'\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    art_date = soup.find(itemprop = 'datePublished')['content'].split('T')[0]\n",
    "    art_title = soup.select('.pg-headline')[0].text\n",
    "    art_body = soup.select('#body-text .zn-body__paragraph')\n",
    "\n",
    "    art_paras = {} \n",
    "    for numPara in range(len(art_body)):\n",
    "        art_paras[numPara+1] = art_body[numPara].text.lstrip('•')\n",
    "\n",
    "    art_dict['title'] = art_title\n",
    "    art_dict['url'] = art_url\n",
    "    art_dict['source'] = 'CNN'\n",
    "    art_dict['date'] = art_date\n",
    "    art_dict['content'] = art_paras\n",
    "            \n",
    "    return art_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getArticle(getArticlesList(getCatLinks()[0])[10]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#先不要用\n",
    "def addContent(articlesList):\n",
    "    \n",
    "    arts_dictList = []\n",
    "    \n",
    "    for num in range(len(articlesList)):\n",
    "        art_info = {}\n",
    "        \n",
    "        res = r.get(articlesList[num]['url'])\n",
    "        res.encoding='unicode'\n",
    "        soup = BeautifulSoup(res.text,'lxml')       \n",
    "        art_body = soup.select('#body-text .zn-body__paragraph')\n",
    "        date = soup.select('.pg-rail-tall meta')[1]['content'].split('T')[0]\n",
    "        \n",
    "        para_dict = {} \n",
    "        for numPara in range(len(art_body)):\n",
    "            para_dict[numPara+1] = art_body[numPara].text.lstrip('•')\n",
    "        \n",
    "        art_info['title'] = articlesList[num]['title']\n",
    "        art_info['date'] = date\n",
    "        art_info['source'] = 'CNN'\n",
    "        art_info['url'] = articlesList[num]['url']\n",
    "        art_info['content'] = para_dict\n",
    "        \n",
    "        arts_dictList.append(art_info)\n",
    "            \n",
    "    return arts_dictList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = r.get('http://edition.cnn.com/2017/04/15/us/atlanta-sun-dial-restaurant-boy-dies/index.html')\n",
    "res.encoding='unicode'\n",
    "soup = BeautifulSoup(res.text,'lxml')       \n",
    "date = soup.select('.pg-rail-tall meta')[1]['content'].split('T')[0]\n",
    "\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "addContent(getArticlesList(getCatLinks()[0])[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jsonOut(inputList):\n",
    "    src = inputList[0]['source']\n",
    "    day = d.today().strftime('%Y%m%d')\n",
    "    filename = '{}_{}_output.json'.format(day,src)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(inputList, f)\n",
    "        f.close()\n",
    "    print(filename + ' has been created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jsonOut(addContent(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
